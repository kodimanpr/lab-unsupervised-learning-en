{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Before-your-start:\" data-toc-modified-id=\"Before-your-start:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Before your start:</a></span></li><li><span><a href=\"#Challenge-1---Import-and-Describe-the-Dataset\" data-toc-modified-id=\"Challenge-1---Import-and-Describe-the-Dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Challenge 1 - Import and Describe the Dataset</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Explore-the-dataset-with-mathematical-and-visualization-techniques.-What-do-you-find?\" data-toc-modified-id=\"Explore-the-dataset-with-mathematical-and-visualization-techniques.-What-do-you-find?-2.0.0.1\"><span class=\"toc-item-num\">2.0.0.1&nbsp;&nbsp;</span>Explore the dataset with mathematical and visualization techniques. What do you find?</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-2---Data-Cleaning-and-Transformation\" data-toc-modified-id=\"Challenge-2---Data-Cleaning-and-Transformation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Challenge 2 - Data Cleaning and Transformation</a></span></li><li><span><a href=\"#Challenge-3---Data-Preprocessing\" data-toc-modified-id=\"Challenge-3---Data-Preprocessing-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Challenge 3 - Data Preprocessing</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#We-will-use-the-StandardScaler-from-sklearn.preprocessing-and-scale-our-data.-Read-more-about-StandardScaler-here.\" data-toc-modified-id=\"We-will-use-the-StandardScaler-from-sklearn.preprocessing-and-scale-our-data.-Read-more-about-StandardScaler-here.-4.0.0.1\"><span class=\"toc-item-num\">4.0.0.1&nbsp;&nbsp;</span>We will use the <code>StandardScaler</code> from <code>sklearn.preprocessing</code> and scale our data. Read more about <code>StandardScaler</code> <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\" target=\"_blank\">here</a>.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-4---Data-Clustering-with-K-Means\" data-toc-modified-id=\"Challenge-4---Data-Clustering-with-K-Means-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Challenge 4 - Data Clustering with K-Means</a></span></li><li><span><a href=\"#Challenge-5---Data-Clustering-with-DBSCAN\" data-toc-modified-id=\"Challenge-5---Data-Clustering-with-DBSCAN-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Challenge 5 - Data Clustering with DBSCAN</a></span></li><li><span><a href=\"#Challenge-6---Compare-K-Means-with-DBSCAN\" data-toc-modified-id=\"Challenge-6---Compare-K-Means-with-DBSCAN-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Challenge 6 - Compare K-Means with DBSCAN</a></span></li><li><span><a href=\"#Bonus-Challenge-2---Changing-K-Means-Number-of-Clusters\" data-toc-modified-id=\"Bonus-Challenge-2---Changing-K-Means-Number-of-Clusters-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Bonus Challenge 2 - Changing K-Means Number of Clusters</a></span></li><li><span><a href=\"#Bonus-Challenge-3---Changing-DBSCAN-eps-and-min_samples\" data-toc-modified-id=\"Bonus-Challenge-3---Changing-DBSCAN-eps-and-min_samples-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Bonus Challenge 3 - Changing DBSCAN <code>eps</code> and <code>min_samples</code></a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before your start:\n",
    "- Read the README.md file\n",
    "- Comment as much as you can and use the resources in the README.md file\n",
    "- Happy learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries:\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings                                              \n",
    "from sklearn.exceptions import DataConversionWarning          \n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1 - Import and Describe the Dataset\n",
    "\n",
    "In this lab, we will use a dataset containing information about customer preferences. We will look at how much each customer spends in a year on each subcategory in the grocery store and try to find similarities using clustering.\n",
    "\n",
    "The origin of the dataset is [here](https://archive.ics.uci.edu/ml/datasets/wholesale+customers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data: Wholesale customers data\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "file_path = '../data/Wholesale customers data.csv'\n",
    "wholesale_data = pd.read_csv(file_path) #pd. is pandas module and need to be imported to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistical summary\n",
    "print(wholesale_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the dataset with mathematical and visualization techniques. What do you find?\n",
    "\n",
    "Checklist:\n",
    "\n",
    "* What does each column mean?\n",
    "* Any categorical data to convert?\n",
    "* Any missing data to remove?\n",
    "* Column collinearity - any high correlations?\n",
    "* Descriptive statistics - any outliers to remove?\n",
    "* Column-wise data distribution - is the distribution skewed?\n",
    "* Etc.\n",
    "\n",
    "Additional info: Over a century ago, an Italian economist named Vilfredo Pareto discovered that roughly 20% of the customers account for 80% of the typical retail sales. This is called the [Pareto principle](https://en.wikipedia.org/wiki/Pareto_principle). Check if this dataset displays this characteristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Understand the dataset\n",
    "\n",
    "# Display data types and unique values for each column\n",
    "print(wholesale_data.dtypes)\n",
    "print(wholesale_data.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataset to understand its structure\n",
    "print(wholesale_data.head())\n",
    "\n",
    "# Summary information about the dataset, including data types and missing values\n",
    "print(wholesale_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Analyze categorical columns\n",
    "# Analyze the distribution of the 'Channel' column\n",
    "print(wholesale_data['Channel'].value_counts())\n",
    "\n",
    "# Analyze the distribution of the 'Region' column\n",
    "print(wholesale_data['Region'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Check for missing data\n",
    "# Display the count of missing values in each column\n",
    "print(wholesale_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Analyze collinearity between columns\n",
    "# Visualize the correlation matrix using a heatmap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "correlation = wholesale_data.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Analyze descriptive statistics and detect outliers\n",
    "# Use boxplot to identify potential outliers in numerical columns\n",
    "wholesale_data.boxplot(figsize=(12, 6), rot=90)\n",
    "plt.title(\"Boxplot of Numerical Columns\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Check the distribution of data in each column\n",
    "# Plot histograms for each numerical column to see their distributions\n",
    "wholesale_data.hist(bins=30, figsize=(12, 10), layout=(3, 3))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Pareto principle analysis (80/20 rule)\n",
    "# Calculate the total sales for each customer\n",
    "wholesale_data['Total_Sales'] = wholesale_data.iloc[:, 2:].sum(axis=1)\n",
    "\n",
    "# Sort the dataset by total sales in descending order\n",
    "pareto_data = wholesale_data.sort_values(by='Total_Sales', ascending=False)\n",
    "\n",
    "# Calculate cumulative percentage of total sales\n",
    "pareto_data['Cumulative_Percentage'] = pareto_data['Total_Sales'].cumsum() / pareto_data['Total_Sales'].sum() * 100\n",
    "\n",
    "# Check if 20% of customers generate 80% of the total sales\n",
    "top_20_percent = pareto_data.head(int(len(pareto_data) * 0.2))\n",
    "print(\"Maximum cumulative percentage for top 20% customers:\", top_20_percent['Cumulative_Percentage'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your observations here**\n",
    "\n",
    "1. Channel and Region as categorical variables: Although these columns are represented as int64, they are categorical in nature. The Channel column likely represents different sales channels (e.g., Horeca or Retail), and the Region column encodes geographical regions. These numbers do not have a numerical meaning but rather act as labels for categories. This distinction is crucial, as these columns should be treated differently during analysis or transformation (e.g., one-hot encoding instead of scaling).\n",
    "\n",
    "2. Column meanings and distributions:\n",
    "- Channel and Region are categorical columns with 2 and 3 unique values, respectively.\n",
    "- Columns such as Fresh, Milk, Grocery, Frozen, Detergents_Paper, and Delicassen represent various product categories, all with significant variation in their values (as seen in the histograms and descriptive statistics).\n",
    "\n",
    "3. Missing data:\n",
    "- No missing values were detected in any column, ensuring data completeness.\n",
    "\n",
    "4. Correlations:\n",
    "- High correlation between Grocery and Detergents_Paper (0.92) and between Milk and Grocery (0.73). This indicates possible collinearity.\n",
    "- Low or negligible correlations between other product categories.\n",
    "\n",
    "5. Outliers:\n",
    "- Boxplots reveal the presence of significant outliers in all numerical columns, especially in Fresh, Milk, and Grocery.\n",
    "\n",
    "6. Skewed distributions:\n",
    "- Histograms show that most columns are heavily right-skewed, with the majority of the data concentrated in lower ranges.\n",
    "\n",
    "7. Pareto principle (80/20 rule):\n",
    "- The top 20% of customers account for approximately 42.9% of total sales, which does not strictly follow the Pareto principle (80/20).\n",
    "\n",
    "**Insights:**\n",
    "\n",
    "- The skewness indicates that the data is not evenly distributed and may require transformations (e.g., logarithmic scaling) to reduce skewness for better analysis and model performance.\n",
    "- The presence of outliers suggests the need for further investigation to determine if these values represent exceptional cases or errors.\n",
    "- The histograms highlight differences in customer behavior, such as small purchases for certain categories, which could be useful for customer segmentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2 - Data Cleaning and Transformation\n",
    "\n",
    "If your conclusion from the previous challenge is the data need cleaning/transformation, do it in the cells below. However, if your conclusion is the data need not be cleaned or transformed, feel free to skip this challenge. But if you do choose the latter, please provide rationale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "#Step 1: Handle Outliers (Optional)\n",
    "\n",
    "# Remove outliers using the IQR method\n",
    "Q1 = wholesale_data.quantile(0.25)\n",
    "Q3 = wholesale_data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define bounds to detect outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter the data to exclude outliers\n",
    "cleaned_data = wholesale_data[~((wholesale_data < lower_bound) | (wholesale_data > upper_bound)).any(axis=1)]\n",
    "\n",
    "print(f\"Original dataset size: {wholesale_data.shape[0]}\")\n",
    "print(f\"Cleaned dataset size: {cleaned_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Encode Categorical Columns\n",
    "\n",
    "encoded_data = pd.get_dummies(cleaned_data, columns=['Channel', 'Region'], drop_first=True)\n",
    "\n",
    "print(\"Categorical columns have been encoded:\")\n",
    "print(encoded_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation for Categoricaal features inclusion into Clustering process\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Boxplot for Channel vs numerical columns\n",
    "for col in ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(x=encoded_data['Channel_2'], y=encoded_data[col])\n",
    "    plt.title(f'{col} distribution by Channel')\n",
    "    plt.show()\n",
    "\n",
    "# Boxplot for Region vs numerical columns\n",
    "for col in ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(x=encoded_data['Region_3'], y=encoded_data[col])\n",
    "    plt.title(f'{col} distribution by Region')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your comment here**\n",
    "\n",
    "1. Outlier Handling:\n",
    "- The original dataset contained 440 rows, and after removing outliers using the IQR method, 332 rows remain.\n",
    "- This indicates that 108 rows (approximately 24.5% of the data) were identified as containing outliers and excluded from the cleaned dataset.\n",
    "- Removing these outliers ensures that extreme values do not skew the analysis or clustering results in subsequent steps.\n",
    "\n",
    "2. Categorical Encoding:\n",
    "- The categorical columns Channel and Region were successfully encoded using one-hot encoding.\n",
    "- Two new columns were created: Channel_2 and Region_3, representing the encoded categories while dropping the first category as the baseline (to prevent multicollinearity).\n",
    "- The dataset now includes these additional columns, which are ready for further analysis or machine learning.\n",
    "\n",
    "3. Cleaned Dataset Structure:\n",
    "- The cleaned dataset retains the structure of the original numerical columns (Fresh, Milk, Grocery, etc.) while adding the newly encoded columns (Channel_2, Region_2, Region_3).\n",
    "- This provides a well-prepared dataset for subsequent preprocessing and clustering tasks.\n",
    "\n",
    "**Conclusion:**\n",
    "The cleaning and transformation steps effectively addressed outliers and prepared the categorical variables for analysis. The dataset is now more consistent and ready for preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Evaluation Summary for categorical features inclusion in the Clustering process**\n",
    "\n",
    "1. Observations on Channel:\n",
    "-  Significant differences were observed in numerical columns like Milk, Grocery, and Detergents_Paper based on the Channel (Channel_2).\n",
    "-  These differences suggest that the distribution channel plays a meaningful role in customer purchasing behavior for certain product categories.\n",
    "\n",
    "2. Observations on Region:\n",
    "-  The differences in numerical columns based on Region (Region_3) were less pronounced.\n",
    "-  While there are slight variations in columns like Frozen and Milk, most numerical columns showed similar patterns across regions.\n",
    "\n",
    "3. Decision for Clustering:\n",
    "-  Include Channel: The observed differences justify including Channel as a feature in the clustering process to improve segmentation based on distribution channels.\n",
    "-  Exclude Region: Since Region does not show significant variation in the data, it will be excluded to reduce dimensionality and avoid introducing unnecessary noise into the clustering process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3 - Data Preprocessing\n",
    "\n",
    "One problem with the dataset is the value ranges are remarkably different across various categories (e.g. `Fresh` and `Grocery` compared to `Detergents_Paper` and `Delicassen`). If you made this observation in the first challenge, you've done a great job! This means you not only completed the bonus questions in the previous Supervised Learning lab but also researched deep into [*feature scaling*](https://en.wikipedia.org/wiki/Feature_scaling). Keep on the good work!\n",
    "\n",
    "Diverse value ranges in different features could cause issues in our clustering. The way to reduce the problem is through feature scaling. We'll use this technique again with this dataset.\n",
    "\n",
    "#### We will use the `StandardScaler` from `sklearn.preprocessing` and scale our data. Read more about `StandardScaler` [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler).\n",
    "\n",
    "*After scaling your data, assign the transformed data to a new variable `customers_scale`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your import here:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Select numerical columns for scaling\n",
    "numerical_columns = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']\n",
    "\n",
    "# Apply StandardScaler to numerical columns\n",
    "scaler = StandardScaler()\n",
    "scaled_numerical_data = scaler.fit_transform(encoded_data[numerical_columns])\n",
    "\n",
    "# Convert scaled numerical data back to a DataFrame\n",
    "customers_scale = pd.DataFrame(scaled_numerical_data, columns=numerical_columns)\n",
    "\n",
    "# Step 2: Include 'Channel' and exclude 'Region'\n",
    "# Add 'Channel_2' to the dataset\n",
    "#customers_scale = pd.concat([scaled_numerical_df, encoded_data[['Channel_2']]], axis=1)\n",
    "\n",
    "print(\"Final dataset prepared for clustering:\")\n",
    "print(customers_scale.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 4 - Data Clustering with K-Means\n",
    "\n",
    "Now let's cluster the data with K-Means first. Initiate the K-Means model, then fit your scaled data. In the data returned from the `.fit` method, there is an attribute called `labels_` which is the cluster number assigned to each data record. What you can do is to assign these labels back to `customers` in a new column called `customers['labels']`. Then you'll see the cluster results of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Your code here:\n",
    "\n",
    "# Step 1: Initialize the K-Means model\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)  # Change n_clusters if necessary\n",
    "\n",
    "# Step 2: Fit the model to the prepared data\n",
    "kmeans.fit(customers_scale)\n",
    "\n",
    "# Step 3: Assign the cluster labels to the dataset\n",
    "customers_scale['Cluster'] = kmeans.labels_\n",
    "\n",
    "print(\"Cluster labels assigned to each record:\")\n",
    "print(customers_scale.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example scatter plot of two features with cluster labels\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(customers_scale['Fresh'], customers_scale['Grocery'], c=customers_scale['Cluster'], cmap='viridis', alpha=0.7)\n",
    "plt.title(\"K-Means Clustering\")\n",
    "plt.xlabel(\"Fresh\")\n",
    "plt.ylabel(\"Grocery\")\n",
    "plt.colorbar(label=\"Cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking to the elbow we can choose 2 like the correct number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_2 = KMeans(n_clusters=2).fit(customers_scale) # Step 1: Initialize K-Means with 2 clusters and Fit the model\n",
    "\n",
    "labels = kmeans_2.predict(customers_scale)\n",
    "\n",
    "clusters = kmeans_2.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_scale['Label'] = clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the values in `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "# Count the number of records in each cluster\n",
    "label_counts = customers_scale['Label'].value_counts()\n",
    "print(\"Number of records in each cluster:\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatter plot for visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(customers_scale['Fresh'], customers_scale['Grocery'], c=customers_scale['Label'], cmap='viridis', alpha=0.7)\n",
    "plt.title(\"K-Means Clustering with 2 Clusters\")\n",
    "plt.xlabel(\"Fresh\")\n",
    "plt.ylabel(\"Grocery\")\n",
    "plt.colorbar(label=\"Cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clusters Stats details\n",
    "\n",
    "cluster_stats = customers_scale.groupby('Label').mean()\n",
    "print(\"Cluster statistics:\")\n",
    "print(cluster_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying all columns with pairs of scatter plots\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pair plot of all numerical columns with cluster labels\n",
    "sns.pairplot(customers_scale, vars=['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen'], \n",
    "             hue='Label', palette='viridis', diag_kind='kde')\n",
    "plt.suptitle(\"Pair Plot of Features by Cluster\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplots for each column grouped by cluster\n",
    "\n",
    "# Boxplots for each numerical column grouped by clusters\n",
    "for col in ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(x='Label', y=col, data=customers_scale, palette='viridis')\n",
    "    plt.title(f'{col} Distribution by Cluster')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensionality reduction (optional)\n",
    "# If you want a global visualization of the data in multiple dimensions, you can apply techniques such as \n",
    "# PCA (Principal Component Analysis) or t-SNE to reduce the dimensions to 2D or 3D and then visualize the clusters.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA to reduce dimensions\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(customers_scale.drop(columns=['Label']))\n",
    "customers_scale['PCA1'] = pca_result[:, 0]\n",
    "customers_scale['PCA2'] = pca_result[:, 1]\n",
    "\n",
    "# Scatter plot of PCA results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(customers_scale['PCA1'], customers_scale['PCA2'], c=customers_scale['Label'], cmap='viridis', alpha=0.7)\n",
    "plt.title(\"PCA Visualization of Clusters\")\n",
    "plt.xlabel(\"PCA1\")\n",
    "plt.ylabel(\"PCA2\")\n",
    "plt.colorbar(label=\"Cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 5 - Data Clustering with DBSCAN\n",
    "\n",
    "Now let's cluster the data using DBSCAN. Use `DBSCAN(eps=0.5)` to initiate the model, then fit your scaled data. In the data returned from the `.fit` method, assign the `labels_` back to `customers['labels_DBSCAN']`. Now your original data have two labels, one from K-Means and the other from DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN \n",
    "\n",
    "# Your code here\n",
    "\n",
    "# Step 1: Initialize the DBSCAN model with eps=0.5\n",
    "dbscan = DBSCAN(eps=0.5)  # Adjust min_samples if needed\n",
    "\n",
    "# Step 2: Fit the model to the scaled dataset\n",
    "dbscan_labels = dbscan.fit_predict(customers_scale)\n",
    "\n",
    "# Step 3: Assign DBSCAN labels to the dataset\n",
    "customers_scale['labels_DBSCAN'] = dbscan_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the values in `labels_DBSCAN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "#Count the number of points in each cluster\n",
    "dbscan_counts = customers_scale['labels_DBSCAN'].value_counts()\n",
    "print(\"DBSCAN Cluster Counts:\")\n",
    "print(dbscan_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 6 - Compare K-Means with DBSCAN\n",
    "\n",
    "Now we want to visually compare how K-Means and DBSCAN have clustered our data. We will create scatter plots for several columns. For each of the following column pairs, plot a scatter plot using `labels` and another using `labels_DBSCAN`. Put them side by side to compare. Which clustering algorithm makes better sense?\n",
    "\n",
    "Columns to visualize:\n",
    "\n",
    "* `Detergents_Paper` as X and `Milk` as y\n",
    "* `Grocery` as X and `Fresh` as y\n",
    "* `Frozen` as X and `Delicassen` as y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Detergents_Paper` as X and `Milk` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def plot(x,y,hue):\n",
    "    sns.scatterplot(x=x, \n",
    "                    y=y,\n",
    "                    hue=hue)\n",
    "    plt.title('Detergents Paper vs Milk ')\n",
    "    return plt.show();\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# corrected function\n",
    "def plot(x, y, hue, data):\n",
    "    sns.scatterplot(x=x, y=y, hue=hue, data=data, palette=\"viridis\", alpha=0.7)\n",
    "    plt.title(f'{x} vs {y} by {hue}')\n",
    "    plt.show()\n",
    "\n",
    "# Visualization\n",
    "plot('Detergents_Paper', 'Milk', 'Label', customers_scale)  # For K-Means\n",
    "plot('Detergents_Paper', 'Milk', 'labels_DBSCAN', customers_scale)  # For DBSCAN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Grocery` as X and `Fresh` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "plot('Grocery', 'Fresh', 'Label', customers_scale)  # For K-Means\n",
    "plot('Grocery', 'Fresh', 'labels_DBSCAN', customers_scale)  # For DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Frozen` as X and `Delicassen` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "plot('Frozen', 'Delicassen', 'Label', customers_scale)  # For K-Means\n",
    "plot('Frozen', 'Delicassen', 'labels_DBSCAN', customers_scale)  # For DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a groupby to see how the mean differs between the groups. Group `customers` by `labels` and `labels_DBSCAN` respectively and compute the means for all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# Group by K-Means labels and calculate the mean\n",
    "kmeans_means = customers_scale.groupby('Label').mean()\n",
    "print(\"Means by K-Means clusters:\")\n",
    "print(kmeans_means)\n",
    "\n",
    "# Group by DBSCAN labels and calculate the mean\n",
    "dbscan_means = customers_scale.groupby('labels_DBSCAN').mean()\n",
    "print(\"\\nMeans by DBSCAN clusters:\")\n",
    "print(dbscan_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which algorithm appears to perform better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your observations here**\n",
    "\n",
    "DBSCAN appears to perform better for this dataset. Here’s why:\n",
    "\n",
    "- Handling Outliers: DBSCAN effectively separates noise (-1) from the main clusters, which helps in preserving the integrity of the meaningful clusters. K-Means, on the other hand, incorporates outliers into the nearest clusters, potentially distorting their characteristics.\n",
    "\n",
    "- Cluster Shape and Density: DBSCAN excels with non-spherical and irregularly shaped clusters, as shown by the variability in means across clusters. K-Means assumes spherical clusters, which can lead to inaccuracies in datasets with irregular patterns.\n",
    "\n",
    "- Noise Representation: The inclusion of a -1 cluster by DBSCAN provides a clear distinction for points that do not belong to any dense cluster, making it more suitable for noisy datasets.\n",
    "\n",
    "- Cluster Means: DBSCAN clusters exhibit distinct means across features, suggesting that it effectively captured different group patterns without being influenced by outliers.\n",
    "\n",
    "However, if the dataset primarily contains spherical clusters with minimal noise, K-Means may still be more efficient due to its simplicity and faster computation. For this specific dataset, DBSCAN's ability to handle noise and irregular cluster shapes makes it the better-performing algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 2 - Changing K-Means Number of Clusters\n",
    "\n",
    "As we mentioned earlier, we don't need to worry about the number of clusters with DBSCAN because it automatically decides that based on the parameters we send to it. But with K-Means, we have to supply the `n_clusters` param (if you don't supply `n_clusters`, the algorithm will use `8` by default). You need to know that the optimal number of clusters differs case by case based on the dataset. K-Means can perform badly if the wrong number of clusters is used.\n",
    "\n",
    "In advanced machine learning, data scientists try different numbers of clusters and evaluate the results with statistical measures (read [here](https://en.wikipedia.org/wiki/Cluster_analysis#External_evaluation)). We are not using statistical measures today but we'll use our eyes instead. In the cells below, experiment with different number of clusters and visualize with scatter plots. What number of clusters seems to work best for K-Means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to visualize K-Means clusters\n",
    "def visualize_kmeans_clusters(n_clusters, data, x_col, y_col):\n",
    "    \"\"\"\n",
    "    Visualize the K-Means clustering results for a specific number of clusters.\n",
    "\n",
    "    Parameters:\n",
    "    - n_clusters: Number of clusters for K-Means\n",
    "    - data: DataFrame containing the dataset\n",
    "    - x_col: Name of the column to use for the x-axis\n",
    "    - y_col: Name of the column to use for the y-axis\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=data[x_col], y=data[y_col], hue=labels, palette=\"viridis\", alpha=0.7)\n",
    "    plt.title(f\"K-Means Clustering with {n_clusters} Clusters\")\n",
    "    plt.xlabel(x_col)\n",
    "    plt.ylabel(y_col)\n",
    "    plt.legend(title=\"Cluster\", loc=\"upper right\", bbox_to_anchor=(1.2, 1))\n",
    "    plt.show()\n",
    "\n",
    "# Iterate over a range of cluster numbers and visualize the results\n",
    "for n in range(2, 6):  # Experiment with cluster numbers from 2 to 5\n",
    "    visualize_kmeans_clusters(n_clusters=n, data=customers_scale, x_col=\"Detergents_Paper\", y_col=\"Milk\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your comment here**\n",
    "\n",
    "**Observations:**\n",
    "-  With 2 clusters, the data points are broadly separated, but some variability within the groups remains unaccounted for.\n",
    "-  With 3 clusters, the separation improves significantly, capturing distinct patterns in the data and providing a clearer boundary between clusters.\n",
    "-  With 4 or more clusters, the distinctions become too granular, leading to potential overfitting and capturing noise rather than meaningful groupings.\n",
    "\n",
    "The visualization with 3 clusters strikes a balance between simplicity and capturing the underlying structure of the data effectively. Therefore, the number of clusters that seems to work best for K-Means appears to be 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 3 - Changing DBSCAN `eps` and `min_samples`\n",
    "\n",
    "Experiment changing the `eps` and `min_samples` params for DBSCAN. See how the results differ with scatter plot visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define a function to visualize clusters\n",
    "def visualize_dbscan(data, x_col, y_col, labels_col, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(\n",
    "        x=data[x_col],\n",
    "        y=data[y_col],\n",
    "        hue=data[labels_col],\n",
    "        palette=\"viridis\",\n",
    "        style=data[labels_col],\n",
    "        alpha=0.7\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_col)\n",
    "    plt.ylabel(y_col)\n",
    "    plt.legend(title=labels_col, loc='best')\n",
    "    plt.show()\n",
    "\n",
    "# Experiment with different eps and min_samples\n",
    "params = [\n",
    "    {'eps': 0.5, 'min_samples': 5},\n",
    "    {'eps': 1, 'min_samples': 5},\n",
    "    {'eps': 1, 'min_samples': 3},\n",
    "    {'eps': 1.5, 'min_samples': 10}\n",
    "]\n",
    "\n",
    "for i, param in enumerate(params):\n",
    "    dbscan = DBSCAN(eps=param['eps'], min_samples=param['min_samples'])\n",
    "    customers_scale[f'labels_DBSCAN_{i}'] = dbscan.fit_predict(customers_scale)\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_dbscan(\n",
    "        customers_scale, \n",
    "        x_col='Detergents_Paper', \n",
    "        y_col='Milk', \n",
    "        labels_col=f'labels_DBSCAN_{i}', \n",
    "        title=f\"DBSCAN Clustering with eps={param['eps']} and min_samples={param['min_samples']}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your comment here**\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "1. Default Parameters (eps=0.5, min_samples=5):\n",
    "- A significant portion of data points were classified as noise (-1 label), suggesting that the density criteria were strict.\n",
    "- Only a small number of clusters formed, which indicates that the dataset's density may not support many tight clusters under these parameters.\n",
    "- Analysis for efficiency:\n",
    "    - High noise (-1 label), with most points not classified into clusters.\n",
    "    - The strict density threshold captures only the densest parts of the dataset. \n",
    "    - Not efficient as it ignores a large portion of the data.\n",
    "\n",
    "2. Increased Eps (eps=1, min_samples=5):\n",
    "- Increasing eps allowed more points to be grouped into clusters, reducing the noise level.\n",
    "- Two additional clusters were identified compared to the default parameters.\n",
    "- This demonstrates how a larger neighborhood radius can connect more points, forming larger or additional clusters.\n",
    "- Analysis for efficiency:\n",
    "    - Noise significantly reduced.\n",
    "    - Larger clusters formed, covering more points.\n",
    "    - Cluster boundaries seem reasonable for the dataset.\n",
    "    - Efficient and balanced, as it captures more points while maintaining meaningful clusters.\n",
    "\n",
    "3. Reduced min_samples (eps=1, min_samples=3):\n",
    "- Decreasing the minimum number of samples further reduced noise points and significantly increased the number of clusters.\n",
    "- Many small clusters appeared, some of which may not have meaningful density patterns, highlighting the risk of setting min_samples too low.\n",
    "- Analysis for efficiency:\n",
    "    - Numerous small clusters formed, suggesting overfitting to minor variations.\n",
    "    - Some clusters may lack meaningful density patterns, leading to fragmented clustering.\n",
    "    - Not efficient, as it creates many trivial clusters.\n",
    "\n",
    "4. Higher Eps and min_samples (eps=1.5, min_samples=10):\n",
    "- A stricter density criterion with higher min_samples led to fewer clusters and more noise points compared to other configurations.\n",
    "- The formed clusters appeared larger, emphasizing that these parameters favor compact and dense clusters.\n",
    "- Analysis for efficiency:\n",
    "    - A more conservative approach with fewer clusters and more noise.\n",
    "    - Captures only the densest regions while leaving a significant portion as noise.\n",
    "    - Partially efficient, but it may miss important patterns in the data.\n",
    "\n",
    "**Summary of Parameter Impact:**\n",
    "- Increasing eps: Leads to larger clusters or a higher number of clusters because it allows more points to be considered neighbors.\n",
    "- Reducing min_samples: Increases the number of clusters by reducing the density threshold, but may create overly small and less meaningful clusters.\n",
    "- Combining High eps with High min_samples: Produces well-defined and compact clusters, though at the cost of classifying more points as noise.\n",
    "\n",
    "Each parameter combination provides insights into the dataset's density distribution, and the choice depends on the clustering objectives.\n",
    "For this dataset, a balance between eps and min_samples should be chosen based on domain knowledge or evaluation metrics.\n",
    "\n",
    "**Conclusion:**\n",
    "The most efficient configuration is \"eps=1 and min_samples=5\", as it provides a balance between:\n",
    "- Reducing noise,\n",
    "- Forming meaningful clusters,\n",
    "- And minimizing the risk of overfitting or underfitting.\n",
    "\n",
    "This configuration ensures the dataset is well-represented without excessive fragmentation or overly restrictive clustering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
